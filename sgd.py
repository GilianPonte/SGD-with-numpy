# -*- coding: utf-8 -*-
"""SGD.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Oi2wBnTb3Uf1-YKywSwIILI_pguKh8Zf
"""

import numpy as np
import matplotlib.pyplot as plt

N = 100000
b = np.ones(N)
x_0 = np.random.normal(0,1, N)

X = np.c_[b, x_0]

## enter 
theta = np.array([1,10])

# calculate y
## Xtheta = 
y = X.dot(theta)
print(len(y))

def gradient_descent(X, y, num_it, lr, batch_size):
  
  # initial weights
  theta_hat = np.random.normal(0,1,2)
  
  # lists to store learning process
  log, mse = [0,0], []
  
  for i in range(0,num_it): 
    # batch_size
    idx = np.random.randint(0, len(X), batch_size)
    Xs = X[idx]
    ys = y[idx]
    n = len(X)

    # predict y
    y_hat = Xs.dot(theta_hat)

    # gradient of e'e w.r.t. theta_hat = (y - y_hat)^2 = gradient of y'y-2*theta_hat'X'y + theta_hat'X'Xtheta_hat w.r.t theta_hat =   
    gradient_w = (-2 * Xs.T.dot(ys) + 2*Xs.T.dot(y_hat))
    
    # weight update
    theta_hat = theta_hat - lr * gradient_w
    
    log = np.vstack((log,theta_hat))
    loss = (y_hat - ys).sum()**2
    mse.append(loss)
    print(i," MSE =", np.round((loss).sum()**2,8), "and theta =", np.round_(theta_hat,1), "and gradient =", np.round(gradient_w,0)) 
  return theta_hat, log, mse

theta_hat, log, mse = gradient_descent(X = X, y = y, num_it = 20, lr = 1e-2, batch_size = 10)

plt.plot(range(len(mse)), mse)
plt.title('Gradient Descent Optimization', fontSize=14)
plt.xlabel('Epochs')
plt.ylabel('MSE')
plt.show()

plt.plot(range(len(log[:,0])), log[:,0])
plt.title('Gradient Descent Optimization', fontSize=14)
plt.xlabel('Epochs')
plt.ylabel('Weights')
plt.show()

# predictions
y_pred = X.dot(theta_hat)

# plot predictions
plt.scatter(y, y_pred)
plt.grid()
plt.xlabel('Actual y')
plt.ylabel('Predicted y')
plt.title('Scatter plot from actual y and predicted y')
print('Mean Squared Error :',(y - y_pred).sum()/N**2)
plt.show()

from mpl_toolkits.mplot3d import Axes3D

def error(X, Y, THETA):
    return np.sum((X.dot(THETA) - Y)**2)/(2*Y.size)

ms = np.linspace(theta[0] - 20 , theta[0] + 20, 20)
bs = np.linspace(theta[1] - 40 , theta[1] + 40, 40)

M, B = np.meshgrid(ms, bs)

zs = np.array([error(xaug, y, theta) 
               for theta in zip(np.ravel(M), np.ravel(B))])
Z = zs.reshape(M.shape)

fig = plt.figure(figsize=(20, 10))
ax = fig.add_subplot(111, projection='3d')

ax.plot_surface(M, B, Z, rstride=1, cstride=1, color='b', alpha=0.2)
#ax.contour(M, B, Z, 20, color='b', alpha=0.5, offset=0, stride=30)


ax.set_xlabel('x1', labelpad=30, fontsize=24, fontweight='bold')
ax.set_ylabel('x2', labelpad=30, fontsize=24, fontweight='bold')
ax.set_zlabel('f(x1,x2)', labelpad=30, fontsize=24, fontweight='bold')
ax.view_init(elev=20., azim=30)
ax.plot([theta[0]], [theta[1]], [cost[-1]] , markerfacecolor='r', markeredgecolor='r', marker='o', markersize=7)
ax.plot([history[0][0]], [history[0][1]], [cost[0]] , markerfacecolor='r', markeredgecolor='r', marker='o', markersize=7)


ax.plot([t[0] for t in history], [t[1] for t in history], cost , markerfacecolor='r', markeredgecolor='r', marker='.', markersize=2)
ax.plot([t[0] for t in history], [t[1] for t in history], 0 , markerfacecolor='r', markeredgecolor='r', marker='.', markersize=2)

fig.suptitle("Minimizando f(x1,x2)", fontsize=24, fontweight='bold')
plt.savefig("Minimization_image.png")