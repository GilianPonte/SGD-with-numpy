# -*- coding: utf-8 -*-
"""SGD.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Oi2wBnTb3Uf1-YKywSwIILI_pguKh8Zf
"""

import numpy as np
import matplotlib.pyplot as plt

N = 100000
b = np.ones(N)
x_0 = np.random.normal(0,1, N)

X = np.c_[b, x_0]

## enter 
theta = np.array([1,10])

# calculate y
## Xtheta = 
y = X.dot(theta)
print(len(y))

def gradient_descent(X, y, num_it, lr, batch_size):
  
  # initial weights
  theta_hat = np.random.normal(0,1,2)
  
  # lists to store learning process
  log, mse = [0,0], []
  
  for i in range(0,num_it): 
    # batch_size
    idx = np.random.randint(0, len(X), batch_size)
    Xs = X[idx]
    ys = y[idx]
    n = len(X)

    # predict y
    y_hat = Xs.dot(theta_hat)

    # gradient of e'e w.r.t. theta_hat = (y - y_hat)^2 = gradient of y'y-2*theta_hat'X'y + theta_hat'X'Xtheta_hat w.r.t theta_hat =   
    gradient_w = (-2 * Xs.T.dot(ys) + 2*Xs.T.dot(y_hat))
    
    # weight update
    theta_hat = theta_hat - lr * gradient_w
    
    log = np.vstack((log,theta_hat))
    loss = (y_hat - ys).sum()**2
    mse.append(loss)
    print(i," MSE =", np.round((loss).sum()**2,8), "and theta =", np.round_(theta_hat,1), "and gradient =", np.round(gradient_w,0)) 
  return theta_hat, log, mse

theta_hat, log, mse = gradient_descent(X = X, y = y, num_it = 20, lr = 1e-2, batch_size = 10)

plt.plot(range(len(mse)), mse)
plt.title('Gradient Descent Optimization', fontSize=14)
plt.xlabel('Epochs')
plt.ylabel('MSE')
plt.show()

plt.plot(range(len(log[:,0])), log[:,0])
plt.title('Gradient Descent Optimization', fontSize=14)
plt.xlabel('Epochs')
plt.ylabel('Weights')
plt.show()

# predictions
y_pred = X.dot(theta_hat)

# plot predictions
plt.scatter(y, y_pred)
plt.grid()
plt.xlabel('Actual y')
plt.ylabel('Predicted y')
plt.title('Scatter plot from actual y and predicted y')
print('Mean Squared Error :',(y - y_pred).sum()/N**2)
plt.show()